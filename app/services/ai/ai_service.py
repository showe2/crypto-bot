import asyncio
import json
import time
from typing import Dict, Any, Optional, List
from datetime import datetime
from loguru import logger
from pydantic import BaseModel, Field

from app.core.config import get_settings
from app.services.ai.groq_ai_service import groq_llama_service
from app.services.ai.docx_service import docx_service

settings = get_settings()


class AIAnalysisRequest(BaseModel):
    """Request model for AI analysis"""
    token_address: str
    service_responses: Dict[str, Any]
    security_analysis: Dict[str, Any]
    analysis_type: str = "deep"
    timestamp: datetime = Field(default_factory=datetime.utcnow)


class AIAnalysisResponse(BaseModel):
    """Response model for AI analysis"""
    ai_score: float = 0.0
    risk_assessment: str = "unknown"
    recommendation: str = "HOLD"
    confidence: float = 0.0
    key_insights: List[str] = Field(default_factory=list)
    risk_factors: List[str] = Field(default_factory=list)
    stop_flags: List[str] = Field(default_factory=list)
    market_metrics: Dict[str, Any] = Field(default_factory=dict)
    llama_reasoning: str = ""
    processing_time: float = 0.0


class LlamaAIService:
    """Service for Llama 3.0 AI token analysis"""
    
    def __init__(self):
        self.model_name = "llama-3.0-70b-instruct"
        self.max_tokens = 4000
        self.temperature = 0.1
        self.system_prompt = self._build_system_prompt()
    
    def _build_system_prompt(self) -> str:
        """Build comprehensive system prompt for token analysis in Russian"""
        return """–¢—ã —ç–∫—Å–ø–µ—Ä—Ç-–∞–Ω–∞–ª–∏—Ç–∏–∫ Solana —Ç–æ–∫–µ–Ω–æ–≤, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—â–∏–π—Å—è –Ω–∞ –æ—Ü–µ–Ω–∫–µ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω–æ–≤ –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º–∏ –æ–∂–∏–¥–∞–Ω–∏—è–º–∏ –æ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö.

    –§–†–ï–ô–ú–í–û–†–ö –ê–ù–ê–õ–ò–ó–ê:
    –û—Ü–µ–Ω–∏–≤–∞–π —Ç–æ–∫–µ–Ω—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–∏—Ö –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç—Ä–∏–∫, –Ω–æ —É—á–∏—Ç—ã–≤–∞–π –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö:

    –†–´–ù–û–ß–ù–ê–Ø –ö–ê–ü–ò–¢–ê–õ–ò–ó–ê–¶–ò–Ø (MCAP):
    - –û—Ç–ª–∏—á–Ω–æ: <$1M (–≤—ã—Å–æ–∫–∏–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª —Ä–æ—Å—Ç–∞)
    - –•–æ—Ä–æ—à–æ: $1M-$10M (—É–º–µ—Ä–µ–Ω–Ω—ã–π —Ä–æ—Å—Ç)
    - –ü—Ä–∏–µ–º–ª–µ–º–æ: $10M-$50M (—É—Å—Ç–æ—è–≤—à–∏–π—Å—è)
    - –ü–ª–æ—Ö–æ: >$50M (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π —Ä–æ—Å—Ç)

    –õ–ò–ö–í–ò–î–ù–û–°–¢–¨:
    - –û—Ç–ª–∏—á–Ω–æ: $500K+ (–æ—á–µ–Ω—å —Å–∏–ª—å–Ω–∞—è)
    - –•–æ—Ä–æ—à–æ: $100K-$500K (—Å–∏–ª—å–Ω–∞—è)
    - –ü—Ä–∏–µ–º–ª–µ–º–æ: $50K-$100K (—É–º–µ—Ä–µ–Ω–Ω–∞—è)
    - –ü–ª–æ—Ö–æ: <$50K (—Å–ª–∞–±–∞—è)

    –û–¢–ù–û–®–ï–ù–ò–ï –û–ë–™–ï–ú/–õ–ò–ö–í–ò–î–ù–û–°–¢–¨:
    - –û—Ç–ª–∏—á–Ω–æ: >10% (–æ—á–µ–Ω—å –∞–∫—Ç–∏–≤–Ω–æ)
    - –•–æ—Ä–æ—à–æ: 5-10% (–∞–∫—Ç–∏–≤–Ω–æ)
    - –ü—Ä–∏–µ–º–ª–µ–º–æ: 1-5% (—É–º–µ—Ä–µ–Ω–Ω–æ)
    - –ü–ª–æ—Ö–æ: <1% (–Ω–∏–∑–∫–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å)

    –ö–û–ù–¶–ï–ù–¢–†–ê–¶–ò–Ø –¢–û–ü–û–í–´–• –•–û–õ–î–ï–†–û–í:
    - –û—Ç–ª–∏—á–Ω–æ: <20% (–æ—Ç–ª–∏—á–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)
    - –•–æ—Ä–æ—à–æ: 20-35% (—Ö–æ—Ä–æ—à–µ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)
    - –ü—Ä–∏–µ–º–ª–µ–º–æ: 35-50% (—É–º–µ—Ä–µ–Ω–Ω—ã–π —Ä–∏—Å–∫)
    - –ü–ª–æ—Ö–æ: >50% (–≤—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫ –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏–∏)

    –°–¢–ê–¢–£–° LP (—Å —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º–∏ –æ–∂–∏–¥–∞–Ω–∏—è–º–∏):
    - –û—Ç–ª–∏—á–Ω–æ: –ü—Ä–æ–≤–µ—Ä–µ–Ω–Ω–æ —Å–æ–∂–∂–µ–Ω/–∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω
    - –•–æ—Ä–æ—à–æ: –°–∏–ª—å–Ω—ã–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
    - –ü—Ä–∏–µ–º–ª–µ–º–æ: –£–º–µ—Ä–µ–Ω–Ω—ã–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –∏–ª–∏ –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è
    - –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ: –î–∞–Ω–Ω—ã–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã (–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ, –Ω–µ –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ)

    –§–õ–ê–ì–ò –ë–ï–ó–û–ü–ê–°–ù–û–°–¢–ò (–¢–û–õ–¨–ö–û –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï):
    - –ê–∫—Ç–∏–≤–Ω—ã–µ –ø–æ–ª–Ω–æ–º–æ—á–∏—è –º–∏–Ω—Ç–∞ (—Ä–∏—Å–∫ –Ω–µ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
    - –ê–∫—Ç–∏–≤–Ω—ã–µ –ø–æ–ª–Ω–æ–º–æ—á–∏—è –∑–∞–º–æ—Ä–æ–∑–∫–∏ (—Ä–∏—Å–∫ –∑–∞–º–æ—Ä–æ–∑–∫–∏ –∞–∫–∫–∞—É–Ω—Ç–æ–≤)
    - –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–µ—Ä–µ–≤–æ–¥–æ–≤ –∏–ª–∏ –ø–æ–≤–µ–¥–µ–Ω–∏–µ honeypot
    - –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–Ω—ã–π rug pull –∏–ª–∏ —Å–∫–∞–º

    –§–ò–õ–û–°–û–§–ò–Ø –î–û–°–¢–£–ü–ù–û–°–¢–ò –î–ê–ù–ù–´–•:
    - –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –ù–ï –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã
    - –§–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–µ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    - –ù–∞–∫–∞–∑—ã–≤–∞–π —Ç–æ–ª—å–∫–æ –∑–∞ —è–≤–Ω–æ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
    - –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ ‚â† –ü–ª–æ—Ö–æ (–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è)

    –§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê:
    –ü—Ä–µ–¥–æ—Å—Ç–∞–≤—å –∞–Ω–∞–ª–∏–∑ –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ —Å:
    - ai_score (0-100): –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ —Ç–æ–∫–µ–Ω–∞
    - risk_assessment: "low", "medium", "high", "critical"
    - recommendation: "BUY", "CONSIDER", "HOLD", "CAUTION", "AVOID"
    - confidence (0-100): –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –∞–Ω–∞–ª–∏–∑–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    - key_insights: –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤
    - risk_factors: –°–ø–∏—Å–æ–∫ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º (–Ω–µ –ø—Ä–æ–±–µ–ª–æ–≤ –≤ –¥–∞–Ω–Ω—ã—Ö)
    - stop_flags: –°–ø–∏—Å–æ–∫ —Ç–æ–ª—å–∫–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∫—Ä–∞—Å–Ω—ã—Ö —Ñ–ª–∞–≥–æ–≤
    - market_metrics: –ö–ª—é—á–µ–≤—ã–µ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    - llama_reasoning: –ü–æ–¥—Ä–æ–±–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ

    –õ–û–ì–ò–ö–ê –ü–†–ò–ù–Ø–¢–ò–Ø –†–ï–®–ï–ù–ò–ô (–ú–µ–Ω–µ–µ —Å—Ç—Ä–æ–≥–∞—è):
    - BUY: –ò—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —Å –≤—ã—Å–æ–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é –≤ –¥–∞–Ω–Ω—ã—Ö
    - CONSIDER: –•–æ—Ä–æ—à–∏–µ –º–µ—Ç—Ä–∏–∫–∏ —Å —Ä–∞–∑—É–º–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    - HOLD: –°–º–µ—à–∞–Ω–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã –∏–ª–∏ —É–º–µ—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    - CAUTION: –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Ç—Ä–µ–≤–æ–∂–Ω—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç
    - AVOID: –Ø–≤–Ω—ã–µ –∫—Ä–∞—Å–Ω—ã–µ —Ñ–ª–∞–≥–∏ –∏–ª–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏

    –†–ê–°–ß–ï–¢ –£–í–ï–†–ï–ù–ù–û–°–¢–ò:
    - –í—ã—Å–æ–∫–∞—è (80-100%): –°–∏–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    - –°—Ä–µ–¥–Ω—è—è (60-79%): –•–æ—Ä–æ—à–µ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å –Ω–µ–∫–æ—Ç–æ—Ä—ã–º–∏ –ø—Ä–æ–±–µ–ª–∞–º–∏
    - –ù–∏–∑–∫–∞—è (40-59%): –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –Ω–æ –±–µ–∑ –∫—Ä–∞—Å–Ω—ã—Ö —Ñ–ª–∞–≥–æ–≤
    - –û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è (<40%): –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ

    –ë—É–¥—å —Ä–µ–∞–ª–∏—Å—Ç–∏—á–µ–Ω –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –∫—Ä–∏–ø—Ç–æ —Ä—ã–Ω–∫–∞—Ö. –§–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞—Ö —Ä–∏—Å–∫–∞, –∞ –Ω–µ –Ω–∞ –ø—Ä–æ–±–µ–ª–∞—Ö –≤ –¥–∞–Ω–Ω—ã—Ö."""
    
    def _prepare_analysis_data(self, request: AIAnalysisRequest) -> Dict[str, Any]:
        """Extract and calculate key metrics from service responses with enhanced data handling"""
        data = {
            "token_address": request.token_address,
            "analysis_type": request.analysis_type
        }
        
        logger.info(f"Extracting enhanced data for {request.token_address}")
        logger.info(f"Available services: {list(request.service_responses.keys())}")
        
        # === MARKET DATA EXTRACTION (Improved) ===
        market_cap = None
        price_usd = None
        price_change_24h = None
        volume_24h = None
        liquidity = None
        
        # Primary: Birdeye
        birdeye_data = request.service_responses.get("birdeye", {})
        if birdeye_data and isinstance(birdeye_data, dict):
            price_data = birdeye_data.get("price", {})
            if price_data and isinstance(price_data, dict):
                try:
                    raw_value = price_data.get("value")
                    if raw_value is not None:
                        price_usd = float(raw_value)
                    
                    raw_mc = price_data.get("market_cap")
                    if raw_mc is not None:
                        market_cap = float(raw_mc)
                    
                    raw_liq = price_data.get("liquidity")
                    if raw_liq is not None:
                        liquidity = float(raw_liq)
                    
                    raw_vol = price_data.get("volume_24h")
                    if raw_vol is not None:
                        volume_24h = float(raw_vol)
                    
                    raw_change = price_data.get("price_change_24h")
                    if raw_change is not None:
                        price_change_24h = float(raw_change)
                    
                    logger.info(f"Birdeye data extracted successfully")
                    
                except (ValueError, TypeError) as e:
                    logger.warning(f"Birdeye data conversion error: {e}")
        
        # Fallback: DexScreener
        if any(x is None for x in [volume_24h, market_cap, liquidity]):
            dex_data = request.service_responses.get("dexscreener", {})
            if dex_data and dex_data.get("pairs", {}).get("pairs"):
                pairs = dex_data["pairs"]["pairs"]
                if isinstance(pairs, list) and len(pairs) > 0:
                    pair = pairs[0]
                    try:
                        if market_cap is None:
                            raw_mc = pair.get("marketCap")
                            market_cap = float(raw_mc) if raw_mc else None
                        
                        if volume_24h is None:
                            vol_data = pair.get("volume", {})
                            raw_vol = vol_data.get("h24") if isinstance(vol_data, dict) else None
                            volume_24h = float(raw_vol) if raw_vol else None
                            
                        if liquidity is None:
                            liq_data = pair.get("liquidity", {})
                            raw_liq = liq_data.get("usd") if isinstance(liq_data, dict) else None
                            liquidity = float(raw_liq) if raw_liq else None
                        
                        logger.info(f"DexScreener fallback applied")
                    except Exception as e:
                        logger.warning(f"DexScreener extraction error: {e}")
        
        # Additional Fallback: SolSniffer for market cap
        if market_cap is None:
            solsniffer_data = request.service_responses.get("solsniffer", {})
            if solsniffer_data and isinstance(solsniffer_data, dict):
                try:
                    # Try both field names that might contain market cap
                    for field_name in ['marketCap', 'market_cap']:
                        raw_mc = solsniffer_data.get(field_name)
                        if raw_mc is not None:
                            market_cap = float(raw_mc)
                            if market_cap > 0:
                                logger.info(f"SolSniffer market cap fallback applied: ${market_cap:,.0f}")
                                break
                    
                except (ValueError, TypeError) as e:
                    logger.warning(f"SolSniffer market cap conversion error: {e}")
        
        data.update({
            "price_usd": price_usd,
            "price_change_24h": price_change_24h,
            "volume_24h": volume_24h,
            "market_cap": market_cap,
            "liquidity": liquidity
        })
        
        # === ENHANCED VOLATILITY EXTRACTION ===
        recent_volatility = None
        if birdeye_data and birdeye_data.get("trades"):
            recent_volatility = self._calculate_simple_volatility(birdeye_data, request.token_address)
        
        data.update({
            "recent_volatility_percent": recent_volatility,
            "volatility_data_available": recent_volatility is not None
        })
        
        # === HOLDER DATA EXTRACTION (Improved with realistic expectations) ===
        holder_count = None
        top_holders_percent = None
        
        # Primary: GOplus
        goplus_data = request.service_responses.get("goplus", {})
        if goplus_data and isinstance(goplus_data, dict):
            logger.info("Processing GOplus holder data...")
            
            # Extract holder count with multiple field attempts
            holder_fields = [
                "holder_count", "holders_count", "holderCount", "totalHolders", 
                "holdersCount", "holder_total", "total_holders"
            ]
            
            for field in holder_fields:
                raw_count = goplus_data.get(field)
                if raw_count is not None:
                    try:
                        if isinstance(raw_count, str):
                            clean = raw_count.replace(",", "").replace(" ", "").lower()
                            if "k" in clean:
                                number_part = clean.replace("k", "")
                                holder_count = int(float(number_part) * 1000)
                            else:
                                holder_count = int(clean) if clean.isdigit() else None
                        elif isinstance(raw_count, (int, float)):
                            holder_count = int(raw_count)
                        
                        if holder_count is not None and holder_count > 0:
                            logger.info(f"Extracted holder count: {holder_count:,} from {field}")
                            break
                            
                    except Exception:
                        continue
            
            # Extract top holders percentage (already extracted in whale analysis, but keep for compatibility)
            holders_array = goplus_data.get("holders")
            if holders_array and isinstance(holders_array, list):
                logger.info(f"Processing {len(holders_array)} holders for distribution analysis...")
                
                top_10_total = 0
                valid_holders = 0
                
                for i, holder in enumerate(holders_array[:10]):
                    if isinstance(holder, dict):
                        percent_fields = ["percent", "percentage", "share", "balance_percent"]
                        
                        for percent_field in percent_fields:
                            percent_raw = holder.get(percent_field)
                            if percent_raw is not None:
                                try:
                                    if isinstance(percent_raw, str):
                                        clean_percent = percent_raw.replace("%", "").replace(",", ".")
                                        percent_val = float(clean_percent)
                                    else:
                                        percent_val = float(percent_raw)
                                    
                                    if 0 <= percent_val <= 100:  # Sanity check
                                        top_10_total += percent_val
                                        valid_holders += 1
                                        break
                                        
                                except Exception:
                                    continue
                
                if valid_holders > 0:
                    top_holders_percent = top_10_total
                    logger.info(f"Top 10 holders: {top_holders_percent:.1f}% ({valid_holders} holders)")
        
        # Fallback sources for holder data
        if holder_count is None:
            # Try RugCheck
            rugcheck_data = request.service_responses.get("rugcheck", {})
            if rugcheck_data and rugcheck_data.get("total_LP_providers"):
                try:
                    holder_count = int(rugcheck_data["total_LP_providers"])
                    logger.info(f"Holder count from RugCheck LP providers: {holder_count}")
                except Exception:
                    pass
            
            # Try SolSniffer
            if holder_count is None:
                solsniffer_data = request.service_responses.get("solsniffer", {})
                if solsniffer_data:
                    for field in ["holderCount", "holder_count", "holders", "totalHolders"]:
                        value = solsniffer_data.get(field)
                        if value:
                            try:
                                holder_count = int(value)
                                logger.info(f"Holder count from SolSniffer: {holder_count}")
                                break
                            except Exception:
                                continue
        
        data.update({
            "holder_count": holder_count,
            "top_holders_percent": top_holders_percent
        })
        
        # === LP STATUS EXTRACTION (More Realistic) ===
        lp_status = "unknown"
        lp_confidence = 0
        lp_evidence = []
        
        # Method 1: RugCheck LP analysis
        rugcheck_data = request.service_responses.get("rugcheck", {})
        if rugcheck_data and rugcheck_data.get("token"):
            # Check for LP lock evidence
            lockers_data = rugcheck_data.get("lockers_data", {})
            if lockers_data and lockers_data.get("lockers"):
                lockers = lockers_data.get("lockers", [])
                if isinstance(lockers, dict) and len(lockers) > 0:
                    total_locked_value = 0
                    for locker_id, locker_info in lockers.items():
                        if isinstance(locker_info, dict):
                            usd_locked = locker_info.get("usdcLocked", 0)
                            if isinstance(usd_locked, (int, float)) and usd_locked > 0:
                                total_locked_value += usd_locked
                    
                    if total_locked_value > 1000:  # Significant value locked
                        lp_status = "locked"
                        lp_confidence = 90
                        lp_evidence.append(f"${total_locked_value:,.0f} locked in Raydium lockers")
                        logger.info(f"LP status: LOCKED (${total_locked_value:,.0f} in lockers)")
            
            # Check burn patterns in top LP holders
            if lp_status == "unknown":
                markets = rugcheck_data.get("market_analysis", {}).get("markets", [])
                for market in markets:
                    if isinstance(market, dict):
                        lp_data = market.get("lp", {})
                        holders = lp_data.get("holders", [])
                        
                        if holders:
                            for holder in holders:
                                if isinstance(holder, dict):
                                    owner = holder.get("owner", "")
                                    percent = holder.get("pct", 0)
                                    
                                    # Check for burn/lock patterns
                                    burn_patterns = ["111111", "dead", "burn", "lock"]
                                    if any(pattern in str(owner).lower() for pattern in burn_patterns):
                                        if percent > 50:
                                            lp_status = "burned"
                                            lp_confidence = 85
                                            lp_evidence.append(f"{percent:.1f}% in burn address")
                                            logger.info(f"LP status: BURNED ({percent:.1f}%)")
                                            break
                
                # High concentration might indicate locking
                if lp_status == "unknown":
                    for market in markets:
                        if isinstance(market, dict):
                            lp_data = market.get("lp", {})
                            holders = lp_data.get("holders", [])
                            
                            if holders and len(holders) > 0:
                                top_holder = holders[0]
                                if isinstance(top_holder, dict):
                                    top_percent = top_holder.get("pct", 0)
                                    if top_percent > 90:
                                        lp_status = "concentrated"
                                        lp_confidence = 60
                                        lp_evidence.append(f"{top_percent:.1f}% concentrated (possibly locked)")

        # Method 2: GOplus LP data
        goplus_data = request.service_responses.get("goplus", {})
        if lp_status == "unknown" and goplus_data:
            lp_holders = goplus_data.get("lp_holders")
            if lp_holders and isinstance(lp_holders, list) and len(lp_holders) > 0:
                for lp_holder in lp_holders:
                    if isinstance(lp_holder, dict):
                        try:
                            percent_raw = lp_holder.get("percent", "0")
                            percent = float(str(percent_raw).replace("%", "").replace(",", ""))
                            
                            if percent > 85:
                                lp_status = "concentrated" 
                                lp_confidence = 50
                                lp_evidence.append(f"LP {percent:.1f}% concentrated")
                                break
                        except Exception:
                            continue
        
        data.update({
            "lp_status": lp_status,
            "lp_confidence": lp_confidence,
            "lp_evidence": lp_evidence
        })
        
        # === SUPPLY AND DEV DATA ===
        total_supply = None
        helius_data = request.service_responses.get("helius", {})
        if helius_data and helius_data.get("supply"):
            supply_info = helius_data["supply"]
            try:
                raw_supply = supply_info.get("ui_amount")
                if raw_supply is not None:
                    total_supply = float(raw_supply)
            except Exception:
                pass
        
        # Dev holdings from RugCheck
        dev_percent = None
        rugcheck_data = request.service_responses.get("rugcheck", {})
        if rugcheck_data and total_supply:
            creator_analysis = rugcheck_data.get("creator_analysis", {})
            if creator_analysis:
                try:
                    creator_balance = float(creator_analysis.get("creator_balance", 0))
                    if creator_balance > 0:
                        dev_percent = (creator_balance / total_supply) * 100
                except Exception:
                    pass
        
        data.update({
            "total_supply": total_supply,
            "dev_percent": dev_percent
        })
        
        # === SECURITY FLAGS (Critical Only) ===
        security_flags = []
        
        # Only include truly critical security issues
        goplus_security = request.service_responses.get("goplus_result", {})
        if goplus_security:
            # Mint authority (unlimited supply)
            mintable = goplus_security.get("mintable", {})
            if isinstance(mintable, dict) and mintable.get("status") == "1":
                security_flags.append("Active mint authority (unlimited supply risk)")
                data["mint_authority_active"] = True
            else:
                data["mint_authority_active"] = False
            
            # Freeze authority (can freeze accounts)
            freezable = goplus_security.get("freezable", {})
            if isinstance(freezable, dict) and freezable.get("status") == "1":
                security_flags.append("Active freeze authority (account freeze risk)")
                data["freeze_authority_active"] = True
            else:
                data["freeze_authority_active"] = False
        
        # Only include verified critical issues from other sources
        critical_issues = request.service_responses.get("critical_issues", [])
        for issue in critical_issues:
            if "rugged" in str(issue).lower() or "scam" in str(issue).lower():
                security_flags.append(issue)
        
        data["security_flags"] = security_flags
        
        # === DERIVED METRICS (Handle None values) ===
        if volume_24h and liquidity and volume_24h > 0 and liquidity > 0:
            data["volume_liquidity_ratio"] = (volume_24h / liquidity) * 100
        else:
            data["volume_liquidity_ratio"] = None
        
        # === DATA QUALITY ASSESSMENT ===
        data_availability = {
            "has_price": price_usd is not None,
            "has_market_cap": market_cap is not None,
            "has_liquidity": liquidity is not None,
            "has_volume": volume_24h is not None,
            "has_holders": holder_count is not None,
            "has_lp_data": lp_status != "unknown",
            "has_volatility": recent_volatility is not None
        }
        
        available_data_count = sum(data_availability.values())
        total_data_points = len(data_availability)
        
        data["data_completeness"] = (available_data_count / total_data_points) * 100
        data["data_availability"] = data_availability
        
        # === STATUS LOGGING ===
        logger.info("Enhanced data extraction summary:")
        logger.info(f"  Market Cap: {f'${market_cap:,.0f}' if market_cap else 'Not available'}")
        logger.info(f"  Liquidity: {f'${liquidity:,.0f}' if liquidity else 'Not available'}")
        logger.info(f"  Volume 24h: {f'${volume_24h:,.0f}' if volume_24h else 'Not available'}")
        logger.info(f"  Volatility: {f'{recent_volatility}%' if recent_volatility else 'Not available'}")
        logger.info(f"  Holder Count: {f'{holder_count:,}' if holder_count else 'Not available'}")
        logger.info(f"  LP Status: {lp_status}")
        logger.info(f"  Security Flags: {len(security_flags)}")
        logger.info(f"  Data Completeness: {data['data_completeness']:.1f}%")

        # === ENHANCED METRICS EXTRACTION ===
        goplus_data = request.service_responses.get("goplus", {})
        rugcheck_data = request.service_responses.get("rugcheck", {})
        
        # Extract whale data
        whale_data = self._extract_whale_data(goplus_data, rugcheck_data)
        data.update({
            "whale_count": whale_data["whale_count"],
            "whale_control_percent": whale_data["whale_control_percent"],
            "top_whale_percent": whale_data["top_whale_percent"]
        })
        
        # Extract sniper data
        sniper_data = self._analyze_sniper_patterns(goplus_data)
        data.update({
            "sniper_similar_holders": sniper_data["similar_holders"],
            "sniper_pattern_detected": sniper_data["pattern_detected"]
        })
        
        logger.info(f"Enhanced metrics: {whale_data['whale_count']} whales, {sniper_data['similar_holders']} sniper patterns")
        
        return data
    
    def _calculate_simple_volatility(self, birdeye_data: Dict[str, Any], token_address: str) -> Optional[float]:
        """Calculate simple volatility from recent trades"""
        try:
            trades_data = birdeye_data.get("trades", {})
            trades = trades_data.get("items", []) if isinstance(trades_data, dict) else trades_data
            
            if not trades or len(trades) < 5:
                logger.info("Insufficient trades data for volatility calculation")
                return None
            
            # Extract prices from recent trades
            prices = []
            for trade in trades[:20]:  # Use up to 20 recent trades
                if isinstance(trade, dict):
                    try:
                        # Check if our token is 'from' or 'to'
                        if trade.get("from", {}).get("address") == token_address:
                            price = float(trade["from"]["price"])
                        elif trade.get("to", {}).get("address") == token_address:
                            price = float(trade["to"]["price"])
                        else:
                            # For your case, token is usually 'from' in buy orders
                            price = float(trade.get("from", {}).get("price", 0))
                        
                        if price > 0:
                            prices.append(price)
                    except (ValueError, TypeError):
                        continue
            
            if len(prices) < 3:
                logger.info(f"Only {len(prices)} valid prices found, need at least 3")
                return None
            
            # Simple volatility = (max_price - min_price) / avg_price * 100
            max_price = max(prices)
            min_price = min(prices)
            avg_price = sum(prices) / len(prices)
            
            volatility = ((max_price - min_price) / avg_price) * 100 if avg_price > 0 else 0
            
            logger.info(f"Volatility calculated: {volatility:.2f}% from {len(prices)} trades")
            return round(volatility, 2)
            
        except Exception as e:
            logger.warning(f"Volatility calculation failed: {e}")
            return None
    
    def _extract_whale_data(self, goplus_data: Dict[str, Any], rugcheck_data: Dict[str, Any]) -> Dict[str, Any]:
        """Extract whale data from existing holder information"""
        try:
            whale_data = {
                "whale_count": 0,
                "whale_control_percent": 0.0,
                "top_whale_percent": 0.0,
                "data_available": False
            }
            
            # Extract from GOplus holders
            holders = goplus_data.get("holders", [])
            if holders and isinstance(holders, list):
                whales = []
                for holder in holders:
                    if isinstance(holder, dict):
                        try:
                            percent_raw = holder.get("percent", "0")
                            percent = float(percent_raw)
                            
                            # Whale threshold: >2% = whale
                            if percent > 2.0:
                                whales.append(percent)
                        except (ValueError, TypeError):
                            continue
                
                if whales:
                    whale_data["whale_count"] = len(whales)
                    whale_data["whale_control_percent"] = round(sum(whales), 2)
                    whale_data["top_whale_percent"] = round(max(whales), 2)
                    whale_data["data_available"] = True
                else:
                    # No whales = good distribution
                    whale_data["data_available"] = True
            
            return whale_data
            
        except Exception as e:
            logger.warning(f"Whale extraction failed: {e}")
            return whale_data

    def _analyze_sniper_patterns(self, goplus_data: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze sniper patterns from holder distribution"""
        try:
            holders = goplus_data.get("holders", [])
            if not holders or len(holders) < 10:
                return {"similar_holders": 0, "pattern_detected": False, "data_available": False}
            
            # Count holders with very similar percentages
            percentages = []
            for holder in holders[:50]:  # Check top 50 holders
                if isinstance(holder, dict):
                    try:
                        percent_raw = holder.get("percent", "0")
                        percent = float(percent_raw)
                        if 0.1 <= percent <= 5.0:  # Sniper range
                            percentages.append(percent)
                    except (ValueError, TypeError):
                        continue
            
            if len(percentages) < 5:
                return {"similar_holders": 0, "pattern_detected": False, "data_available": True}
            
            # Count very similar percentages
            similar_count = 0
            for i, p1 in enumerate(percentages):
                for p2 in percentages[i+1:]:
                    if abs(p1 - p2) < 0.05:  # Very similar percentages
                        similar_count += 1
            
            pattern_detected = similar_count > 5
            
            return {
                "similar_holders": similar_count,
                "pattern_detected": pattern_detected,
                "data_available": True
            }
            
        except Exception as e:
            logger.warning(f"Sniper pattern analysis failed: {e}")
            return {"similar_holders": 0, "pattern_detected": False, "data_available": False}
        
    async def analyze_token_timing(self, request: AIAnalysisRequest) -> Optional[Dict[str, Any]]:
        """Separate timing analysis with Russian predictions"""
        try:
            logger.info(f"‚è∞ Starting timing analysis for {request.token_address}")
            
            # Prepare data
            analysis_data = self._prepare_analysis_data(request)
            
            # Build timing-specific prompt
            timing_prompt = self._build_timing_analysis_prompt(analysis_data)
            
            # Call Groq
            response_text = await groq_llama_service.send_request(timing_prompt)
            
            if not response_text:
                return None
            
            # Parse timing response
            try:
                timing_data = json.loads(response_text)
                logger.info(f"‚úÖ Timing analysis completed")
                return timing_data
            except json.JSONDecodeError as e:
                logger.error(f"Failed to parse timing JSON: {e}")
                return None
                
        except Exception as e:
            logger.error(f"Timing analysis failed: {e}")
            return None

    def _build_timing_analysis_prompt(self, data: Dict[str, Any]) -> str:
        """Build timing-specific analysis prompt"""
        
        # Calculate additional timing indicators
        volume_24h = data.get('volume_24h', 0)
        market_cap = data.get('market_cap', 0)
        price_change_24h = data.get('price_change_24h', 0)
        volatility = data.get('recent_volatility_percent', 0)
        
        # Activity level assessment
        activity_level = "–Ω–∏–∑–∫–∞—è"
        if volume_24h > 100000:
            activity_level = "–≤—ã—Å–æ–∫–∞—è"
        elif volume_24h > 10000:
            activity_level = "—Å—Ä–µ–¥–Ω—è—è"
        
        # Market cap category
        mcap_category = "–∫—Ä—É–ø–Ω–∞—è"
        if market_cap < 1000000:
            mcap_category = "–º–∏–∫—Ä–æ"
        elif market_cap < 10000000:
            mcap_category = "–º–∞–ª–∞—è"
        elif market_cap < 50000000:
            mcap_category = "—Å—Ä–µ–¥–Ω—è—è"
        
        prompt = f"""–ê–ù–ê–õ–ò–ó –í–†–ï–ú–ï–ù–ò –°–û–õ–ê–ù–ê –¢–û–ö–ï–ù–ê

    –¢–û–ö–ï–ù: {data['token_address']}

    === –í–†–ï–ú–ï–ù–ù–´–ï –î–ê–ù–ù–´–ï ===
    –ù–µ–¥–∞–≤–Ω—è—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å: {data.get('recent_volatility_percent', '–ù–µ–¥–æ—Å—Ç—É–ø–Ω–æ')}%
    –û—Ç–Ω–æ—à–µ–Ω–∏–µ –æ–±—ä–µ–º/–ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç—å: {data.get('volume_liquidity_ratio', '–ù–µ–¥–æ—Å—Ç—É–ø–Ω–æ')}%
    –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∏—Ç–æ–≤: {data.get('whale_count', '–ù–µ–¥–æ—Å—Ç—É–ø–Ω–æ')}
    –ö–æ–Ω—Ç—Ä–æ–ª—å –∫–∏—Ç–æ–≤: {data.get('whale_control_percent', '–ù–µ–¥–æ—Å—Ç—É–ø–Ω–æ')}%
    –°—Ö–æ–∂–∏–µ —Ö–æ–ª–¥–µ—Ä—ã (–±–æ—Ç—ã): {data.get('sniper_similar_holders', '–ù–µ–¥–æ—Å—Ç—É–ø–Ω–æ')}
    –û–±—ä–µ–º 24—á: ${volume_24h:,.0f} ({activity_level} –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å)
    –†—ã–Ω–æ—á–Ω–∞—è –∫–∞–ø: ${market_cap:,.0f} ({mcap_category} –∫–∞–ø)
    –õ–∏–∫–≤–∏–¥–Ω–æ—Å—Ç—å: ${data.get('liquidity', 0):,.0f}
    –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ü–µ–Ω—ã 24—á: {price_change_24h:+.2f}%

    === –ê–ù–ê–õ–ò–ó –í–†–ï–ú–ï–ù–ù–´–• –ü–ê–¢–¢–ï–†–ù–û–í ===

    –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫–Ω–∞ –¥–ª—è —ç—Ç–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–µ–¥—É—é—â–∏—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤:

    1. –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û–°–õ–ï–î–ù–ï–ì–û –ü–ê–ú–ü–ê:
    - –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ü–µ–Ω—ã 24—á {price_change_24h:+.2f}% (>20% = –Ω–µ–¥–∞–≤–Ω–∏–π –ø–∞–º–ø)
    - –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å {volatility}% (>30% = –Ω–µ–¥–∞–≤–Ω—è—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å)
    - –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ç–æ—Ä–≥–æ–≤–ª–∏: {activity_level}
    - –ï—Å–ª–∏ –≤—ã—Å–æ–∫–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ = "–ù–µ–¥–∞–≤–Ω–æ" –∏–ª–∏ "24—á –Ω–∞–∑–∞–¥"
    - –ï—Å–ª–∏ –Ω–∏–∑–∫–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ = "–ù–µ—Ç –Ω–µ–¥–∞–≤–Ω–∏—Ö –ø–∞–º–ø–æ–≤"

    2. –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï –°–õ–ï–î–£–Æ–©–ï–ì–û –û–ö–ù–ê:
    - –ú–∏–∫—Ä–æ –∫–∞–ø (<$1M) + –≤—ã—Å–æ–∫–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å = "–ù–µ–º–µ–¥–ª–µ–Ω–Ω–æ" –∏–ª–∏ "1-2—á"
    - –ú–∞–ª–∞—è –∫–∞–ø ($1-10M) + —Å—Ä–µ–¥–Ω—è—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å = "2-6—á" –∏–ª–∏ "6-24—á"
    - –°—Ä–µ–¥–Ω—è—è/–∫—Ä—É–ø–Ω–∞—è –∫–∞–ø + –Ω–∏–∑–∫–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å = "1-3–¥" –∏–ª–∏ "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
    - –í—ã—Å–æ–∫–∞—è –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è –∫–∏—Ç–æ–≤ = –±–æ–ª–µ–µ –¥–æ–ª–≥–∏–µ –æ–∫–Ω–∞
    - –ú–Ω–æ–≥–æ –±–æ—Ç–æ–≤ = –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä—ã–µ –æ–∫–Ω–∞

    3. –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –§–ê–ó–´ –†–´–ù–ö–ê:
    - –ù–∏–∑–∫–∏–π –æ–±—ä–µ–º + —Å—Ç–∞–±–∏–ª—å–Ω–∞—è —Ü–µ–Ω–∞ = "–Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ"
    - –í—ã—Å–æ–∫–∏–π –æ–±—ä–µ–º + —Ä–æ—Å—Ç —Ü–µ–Ω—ã = "–ø–∞–º–ø"
    - –í—ã—Å–æ–∫–∏–π –æ–±—ä–µ–º + –ø–∞–¥–µ–Ω–∏–µ —Ü–µ–Ω—ã = "—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ"
    - –ù–∏–∑–∫–∏–π –æ–±—ä–µ–º + –±–æ–∫–æ–≤–∏–∫ = "–∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è"
    - –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö = "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"

    4. –û–¶–ï–ù–ö–ê –í–ï–†–û–Ø–¢–ù–û–°–¢–ò –ü–ê–ú–ü–ê:
    - –£—á–∏—Ç—ã–≤–∞–π –≤–æ–∑—Ä–∞—Å—Ç —Ç–æ–∫–µ–Ω–∞, –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
    - –ù–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã + –≤—ã—Å–æ–∫–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å = –≤—ã—Å–æ–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
    - –£—Å—Ç–æ—è–≤—à–∏–µ—Å—è —Ç–æ–∫–µ–Ω—ã + –Ω–∏–∑–∫–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å = –Ω–∏–∑–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å

    –í–†–ï–ú–ï–ù–ù–´–ï –ü–†–ê–í–ò–õ–ê:
    - –ù–µ–º–µ–¥–ª–µ–Ω–Ω–æ: –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, –º–∏–∫—Ä–æ –∫–∞–ø, —Å–≤–µ–∂–∏–µ —Å–∏–≥–Ω–∞–ª—ã
    - 1-2—á: –í—ã—Å–æ–∫–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, –Ω–µ–±–æ–ª—å—à–∞—è –∫–∞–ø, –Ω–µ–¥–∞–≤–Ω–∏–π –∏–º–ø—É–ª—å—Å
    - 2-6—á: –°—Ä–µ–¥–Ω—è—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, —É–º–µ—Ä–µ–Ω–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã
    - 6-24—á: –ù–∏–∑–∫–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, –Ω–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –µ—Å—Ç—å
    - 1-3–¥: –î–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã, –∫—Ä—É–ø–Ω—ã–µ –∫–∞–ø—ã
    - –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ: –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–≤—ã–µ —Å–∏–≥–Ω–∞–ª—ã

    –§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (–¢–û–õ–¨–ö–û JSON):
    {{
    "last_pump": "–ù–µ–¥–∞–≤–Ω–æ|24—á –Ω–∞–∑–∞–¥|2-3 –¥–Ω—è –Ω–∞–∑–∞–¥|–ù–µ–¥–µ–ª—é –Ω–∞–∑–∞–¥|–ù–µ—Ç –Ω–µ–¥–∞–≤–Ω–∏—Ö –ø–∞–º–ø–æ–≤",
    "next_window": "–ù–µ–º–µ–¥–ª–µ–Ω–Ω–æ|1-2—á|2-6—á|6-24—á|1-3–¥|–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ",
    "pump_probability": 0-100,
    "timing_confidence": 0-100,
    "market_phase": "–Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ|–ø–∞–º–ø|—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ|–∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è|–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ",
    "reasoning": "–ö—Ä–∞—Ç–∫–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)",
    "signals": ["—Å–ø–∏—Å–æ–∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤"]
    }}

    –ü–†–ò–ù–¶–ò–ü–´ –ê–ù–ê–õ–ò–ó–ê –í–†–ï–ú–ï–ù–ò:
    - –ò—Å–ø–æ–ª—å–∑—É–π –ö–û–ù–ö–†–ï–¢–ù–´–ï –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    - –í—ã—Å–æ–∫–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ü–µ–Ω—ã = –Ω–µ–¥–∞–≤–Ω—è—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
    - –ú–∞–ª–∞—è –∫–∞–ø + –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å = –±—ã—Å—Ç—Ä—ã–µ –æ–∫–Ω–∞
    - –ë–æ–ª—å—à–∞—è –∫–∞–ø + —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å = –º–µ–¥–ª–µ–Ω–Ω—ã–µ –æ–∫–Ω–∞
    - –ë—É–¥—å —Ä–µ–∞–ª–∏—Å—Ç–∏—á–µ–Ω —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é

    –û–¢–í–ï–ß–ê–ô –¢–û–õ–¨–ö–û JSON."""

        return prompt
    
    def _build_main_analysis_prompt(self, data: Dict[str, Any]) -> str:
        """Build main analysis prompt without timing section"""
        
        # Helper function to format data availability
        def format_data_point(value, label, format_func=None):
            if value is not None:
                formatted = format_func(value) if format_func else str(value)
                return f"{label}: {formatted} ‚úì"
            return f"{label}: –ù–µ–¥–æ—Å—Ç—É–ø–Ω–æ"
        
        # Build market data section
        market_data_lines = [
            format_data_point(data.get('market_cap'), "–†—ã–Ω–æ—á–Ω–∞—è –∫–∞–ø", lambda x: f"${x:,.0f}"),
            format_data_point(data.get('liquidity'), "–õ–∏–∫–≤–∏–¥–Ω–æ—Å—Ç—å", lambda x: f"${x:,.0f}"),
            format_data_point(data.get('volume_24h'), "–û–±—ä–µ–º 24—á", lambda x: f"${x:,.0f}"),
            format_data_point(data.get('volume_liquidity_ratio'), "–û–±—ä–µ–º/–õ–∏–∫–≤–∏–¥–Ω–æ—Å—Ç—å", lambda x: f"{x:.1f}%"),
            format_data_point(data.get('price_usd'), "–¶–µ–Ω–∞", lambda x: f"${x:.8f}"),
            format_data_point(data.get('price_change_24h'), "–ò–∑–º–µ–Ω–µ–Ω–∏–µ 24—á", lambda x: f"{x:+.2f}%")
        ]
        
        # Build enhanced metrics section
        enhanced_metrics_lines = [
            format_data_point(data.get('recent_volatility_percent'), "–ù–µ–¥–∞–≤–Ω—è—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å", lambda x: f"{x}%"),
            format_data_point(data.get('whale_count'), "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∏—Ç–æ–≤", lambda x: f"{x} –∫–∏—Ç–æ–≤"),
            format_data_point(data.get('whale_control_percent'), "–ö–æ–Ω—Ç—Ä–æ–ª—å –∫–∏—Ç–æ–≤", lambda x: f"{x}%"),
            format_data_point(data.get('top_whale_percent'), "–¢–æ–ø –∫–∏—Ç", lambda x: f"{x}%"),
            format_data_point(data.get('sniper_similar_holders'), "–°—Ö–æ–∂–∏–µ —Ö–æ–ª–¥–µ—Ä—ã", lambda x: f"{x} –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"),
            f"–û–±–Ω–∞—Ä—É–∂–µ–Ω –ø–∞—Ç—Ç–µ—Ä–Ω —Å–Ω–∞–π–ø–µ—Ä–æ–≤: {data.get('sniper_pattern_detected', False)}"
        ]
        
        # Build holder data section
        holder_data_lines = [
            format_data_point(data.get('holder_count'), "–í—Å–µ–≥–æ —Ö–æ–ª–¥–µ—Ä–æ–≤", lambda x: f"{x:,}"),
            format_data_point(data.get('top_holders_percent'), "–ö–æ–Ω—Ç—Ä–æ–ª—å —Ç–æ–ø-10", lambda x: f"{x:.1f}%"),
            format_data_point(data.get('dev_percent'), "–•–æ–ª–¥–∏–Ω–≥–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞", lambda x: f"{x:.1f}%")
        ]
        
        # Build LP section
        lp_status = data.get('lp_status', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
        lp_confidence = data.get('lp_confidence', 0)
        lp_evidence = data.get('lp_evidence', [])
        
        lp_status_text = {
            'locked': '–ó–ê–©–ò–©–ï–ù (–ó–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω)',
            'burned': '–ó–ê–©–ò–©–ï–ù (–°–æ–∂–∂–µ–Ω)', 
            'concentrated': '–í–ï–†–û–Ø–¢–ù–û –ó–ê–©–ò–©–ï–ù (–°–∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω)',
            'unknown': '–ù–ï–ò–ó–í–ï–°–¢–ù–û (–î–∞–Ω–Ω—ã–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã)'
        }.get(lp_status, '–ù–ï–ò–ó–í–ï–°–¢–ù–û')
        
        lp_info = f"–°—Ç–∞—Ç—É—Å LP: {lp_status_text}"
        if lp_confidence > 0:
            lp_info += f" (–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {lp_confidence}%)"
        if lp_evidence:
            lp_info += f"\n–î–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞: {'; '.join(lp_evidence)}"
        
        # Security flags
        security_flags = data.get('security_flags', [])
        security_section = "–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã" if not security_flags else "\n".join(f"üö® {flag}" for flag in security_flags)
        
        prompt = f"""–†–ê–°–®–ò–†–ï–ù–ù–´–ô –ê–ù–ê–õ–ò–ó SOLANA –¢–û–ö–ï–ù–ê - AI –û–¶–ï–ù–ö–ê –†–ò–°–ö–û–í

    –¢–û–ö–ï–ù: {data['token_address']}

    === –†–´–ù–û–ß–ù–´–ï –ü–û–ö–ê–ó–ê–¢–ï–õ–ò ===
    {chr(10).join(market_data_lines)}

    === –†–ê–°–®–ò–†–ï–ù–ù–´–ï –ú–ï–¢–†–ò–ö–ò –†–ò–°–ö–û–í ===
    {chr(10).join(enhanced_metrics_lines)}

    === –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –•–û–õ–î–ï–†–û–í ===  
    {chr(10).join(holder_data_lines)}

    === –ë–ï–ó–û–ü–ê–°–ù–û–°–¢–¨ –õ–ò–ö–í–ò–î–ù–û–°–¢–ò ===
    {lp_info}
    –ü–æ–ª–Ω–æ–º–æ—á–∏—è –º–∏–Ω—Ç–∞: {'–ê–ö–¢–ò–í–ù–´ üö®' if data.get('mint_authority_active') else '–û–¢–ö–õ–Æ–ß–ï–ù–´ ‚úì'}
    –ü–æ–ª–Ω–æ–º–æ—á–∏—è –∑–∞–º–æ—Ä–æ–∑–∫–∏: {'–ê–ö–¢–ò–í–ù–´ ‚ö†Ô∏è' if data.get('freeze_authority_active') else '–û–¢–ö–õ–Æ–ß–ï–ù–´ ‚úì'}

    === –ê–ù–ê–õ–ò–ó –ë–ï–ó–û–ü–ê–°–ù–û–°–¢–ò ===
    {security_section}

    === –î–û–°–¢–£–ü–ù–û–°–¢–¨ –î–ê–ù–ù–´–• ===
    –û–±—â–∞—è –ø–æ–ª–Ω–æ—Ç–∞: {data.get('data_completeness', 0):.1f}%
    –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ç–æ—á–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {sum(data.get('data_availability', {}).values())} / {len(data.get('data_availability', {}))}

    === –ò–ù–°–¢–†–£–ö–¶–ò–ò –î–õ–Ø AI –ê–ù–ê–õ–ò–ó–ê ===

    –¢—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—à—å —ç—Ç–æ—Ç —Ç–æ–∫–µ–Ω —Å –†–ê–°–®–ò–†–ï–ù–ù–´–ú–ò –ú–ï–¢–†–ò–ö–ê–ú–ò. –û—Ü–µ–Ω–∏ —É—Ä–æ–≤–µ–Ω—å —Ä–∏—Å–∫–∞ –∫–∞–∂–¥–æ–π –º–µ—Ç—Ä–∏–∫–∏:

    1. –û–¶–ï–ù–ö–ê –†–ò–°–ö–ê –†–´–ù–û–ß–ù–û–ô –ö–ê–ü–ò–¢–ê–õ–ò–ó–ê–¶–ò–ò:
    - –û—Ü–µ–Ω–∏, —É–∫–∞–∑—ã–≤–∞–µ—Ç –ª–∏ —Ä—ã–Ω–æ—á–Ω–∞—è –∫–∞–ø –Ω–∞ —Ä–∏—Å–∫ –ø–∞–º–ø–∞, –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª —Ä–æ—Å—Ç–∞ –∏–ª–∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å
    - –†–∞—Å—Å–º–æ—Ç—Ä–∏ —Ä—ã–Ω–æ—á–Ω—É—é –∫–∞–ø –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç–∏ –∏ –æ–±—ä–µ–º–∞

    2. –û–¶–ï–ù–ö–ê –†–ò–°–ö–ê –í–û–õ–ê–¢–ò–õ–¨–ù–û–°–¢–ò:
    - –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –ø—Ä–æ—Ü–µ–Ω—Ç –Ω–µ–¥–∞–≤–Ω–µ–π —Ç–æ—Ä–≥–æ–≤–æ–π –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
    - –í—ã—Å–æ–∫–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –º–æ–∂–µ—Ç —É–∫–∞–∑—ã–≤–∞—Ç—å –Ω–∞ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –ò–õ–ò –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å
    - –†–∞—Å—Å–º–æ—Ç—Ä–∏ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –æ–±—ä–µ–º–∞ –∏ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∫–∏—Ç–æ–≤

    3. –û–¶–ï–ù–ö–ê –†–ò–°–ö–ê –ö–ò–¢–û–í:
    - –û—Ü–µ–Ω–∏ –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—é –∫–∏—Ç–æ–≤ –∏ —Ä–∏—Å–∫ –¥–∞–º–ø–∞
    - 0 –∫–∏—Ç–æ–≤ = –õ–£–ß–®–ò–ô (–∏–¥–µ–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)
    - –†–∞—Å—Å–º–æ—Ç—Ä–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∏—Ç–æ–≤ –ø—Ä–æ—Ç–∏–≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞ –∫–æ–Ω—Ç—Ä–æ–ª—è
    - –û—Ü–µ–Ω–∏ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –∫–æ–æ—Ä–¥–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–æ–¥–∞–∂

    4. –û–¶–ï–ù–ö–ê –†–ò–°–ö–ê –°–ù–ê–ô–ü–ï–†–û–í/–ë–û–¢–û–í:
    - –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ö–æ–ª–¥–µ—Ä–æ–≤ –Ω–∞ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π —Å–ø—Ä–æ—Å
    - –ú–Ω–æ–≥–æ —Å—Ö–æ–∂–∏—Ö –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤ —Ö–æ–ª–¥–µ—Ä–æ–≤ = –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –±–æ—Ç–æ–≤
    - –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –∫–æ–æ—Ä–¥–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ–∫—É–ø–∫–∏

    5. –û–¶–ï–ù–ö–ê –ì–õ–£–ë–ò–ù–´ –õ–ò–ö–í–ò–î–ù–û–°–¢–ò:
    - –û—Ü–µ–Ω–∏ –æ—Ç–Ω–æ—à–µ–Ω–∏–µ –æ–±—ä–µ–º/–ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç—å –¥–ª—è –∑–¥–æ—Ä–æ–≤—å—è —Ä—ã–Ω–∫–∞
    - –í—ã—Å–æ–∫–æ–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–µ = –∞–∫—Ç–∏–≤–Ω–∞—è —Ç–æ—Ä–≥–æ–≤–ª—è, –ù–∏–∑–∫–æ–µ = —Ç–æ–Ω–∫–∏–µ —Ä—ã–Ω–∫–∏
    - –†–∞—Å—Å–º–æ—Ç—Ä–∏ –≥–ª—É–±–∏–Ω—É –ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç–∏ –¥–ª—è –≤–ª–∏—è–Ω–∏—è –Ω–∞ —Ü–µ–Ω—É

    6. –û–¶–ï–ù–ö–ê –†–ò–°–ö–ê –•–û–õ–î–ò–ù–ì–û–í –†–ê–ó–†–ê–ë–û–¢–ß–ò–ö–ê:
    - –û—Ü–µ–Ω–∏ –ø—Ä–æ—Ü–µ–Ω—Ç —Ç–æ–∫–µ–Ω–æ–≤ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞
    - –í—ã—Å–æ–∫–∏–µ —Ö–æ–ª–¥–∏–Ω–≥–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞ = —Ä–∏—Å–∫ –¥–∞–º–ø–∞
    - –†–∞—Å—Å–º–æ—Ç—Ä–∏, —Ä–∞–∑—É–º–Ω—ã –ª–∏ —Ö–æ–ª–¥–∏–Ω–≥–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞ –¥–ª—è —Å—Ç–∞–¥–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞

    7. –û–¶–ï–ù–ö–ê –ë–ï–ó–û–ü–ê–°–ù–û–°–¢–ò LP:
    - –û—Ü–µ–Ω–∏ —Å—Ç–∞—Ç—É—Å –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏/—Å–∂–∏–≥–∞–Ω–∏—è –ø–æ—Å—Ç–∞–≤—â–∏–∫–∞ –ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç–∏
    - –ó–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω/–°–æ–∂–∂–µ–Ω = –±–µ–∑–æ–ø–∞—Å–Ω–æ, –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ = –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ (–Ω–µ –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ)
    - –†–∞—Å—Å–º–æ—Ç—Ä–∏ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ LP –∏ —É—Ä–æ–≤–µ–Ω—å –¥–æ–≤–µ—Ä–∏—è

    –ö–û–ú–ü–õ–ï–ö–°–ù–ê–Ø –û–¶–ï–ù–ö–ê –†–ò–°–ö–û–í:
    - –ù–ï –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä—É–π —Ä–∏—Å–∫–∏ - –∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–∞–∂–¥—É—é –º–µ—Ç—Ä–∏–∫—É –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ
    - –†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–π –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –º–µ—Ç—Ä–∏–∫ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≤—ã—Å–æ–∫–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å + –∫–∏—Ç—ã = –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∏—Å–∫)
    - –í–∑–≤–µ—à–∏–≤–∞–π –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ–≤–µ—Ä–∏—è –∫ –∫–∞—á–µ—Å—Ç–≤—É –¥–∞–Ω–Ω—ã—Ö
    - –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ = –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞, –Ω–µ –Ω–µ–≥–∞—Ç–∏–≤–Ω–∞—è

    –§–û–†–ú–ê–¢ JSON –û–¢–í–ï–¢–ê (–¢–û–õ–¨–ö–û JSON):
    {{
    "ai_score": 0-100,
    "risk_assessment": "low|medium|high|critical",
    "recommendation": "BUY|CONSIDER|HOLD|CAUTION|AVOID", 
    "confidence": 0-100,
    "key_insights": ["–∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã —Å –¥–∞–Ω–Ω—ã–º–∏"],
    "risk_factors": ["–∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã —Å –¥–∞–Ω–Ω—ã–º–∏"],
    "stop_flags": ["—Ç–æ–ª—å–∫–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã"],
    "market_metrics": {{
        "volatility_risk": "low|medium|high|unknown",
        "whale_risk": "low|medium|high|unknown", 
        "sniper_risk": "low|medium|high|unknown",
        "liquidity_health": "excellent|good|poor|unknown",
        "dev_risk": "low|medium|high|unknown",
        "lp_security": "secure|likely_secure|unknown|risky"
    }},
    "llama_reasoning": "–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫"
    }}

    –ö–†–ò–¢–ï–†–ò–ò –†–ï–®–ï–ù–ò–ô:
    - BUY: –ë–∞–ª–ª >85, –≤—Å–µ –æ—Å–Ω–æ–≤–Ω—ã–µ —Ä–∏—Å–∫–∏ –Ω–∏–∑–∫–∏–µ, –≤—ã—Å–æ–∫–æ–µ –¥–æ–≤–µ—Ä–∏–µ –∫ –¥–∞–Ω–Ω—ã–º
    - CONSIDER: –ë–∞–ª–ª >70, –ø—Ä–∏–µ–º–ª–µ–º—ã–µ —É—Ä–æ–≤–Ω–∏ —Ä–∏—Å–∫–æ–≤, —Ö–æ—Ä–æ—à–∏–µ –¥–∞–Ω–Ω—ã–µ
    - HOLD: –ë–∞–ª–ª >55, —Å–º–µ—à–∞–Ω–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã –∏–ª–∏ —É–º–µ—Ä–µ–Ω–Ω—ã–µ —Ä–∏—Å–∫–∏
    - CAUTION: –ë–∞–ª–ª >40, –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Ç—Ä–µ–≤–æ–∂–Ω—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã
    - AVOID: –ë–∞–ª–ª <40 –∏–ª–∏ –ª—é–±—ã–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–ª–∞–≥–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏

    –û–¢–í–ï–ß–ê–ô –¢–û–õ–¨–ö–û –í–ê–õ–ò–î–ù–´–ú JSON."""

        return prompt
    
    def _create_fallback_response(self, token_address: str, processing_time: float) -> AIAnalysisResponse:
        """Create neutral fallback response when AI analysis fails"""
        return AIAnalysisResponse(
            ai_score=60.0,  # Neutral instead of 0
            risk_assessment="medium",  # Medium instead of critical
            recommendation="HOLD",  # Hold instead of avoid
            confidence=50.0,  # Medium confidence
            key_insights=["Analysis system temporarily unavailable"],
            risk_factors=["Limited analysis due to system issue"],
            stop_flags=[],  # No stop flags for system errors
            market_metrics={},
            llama_reasoning="AI analysis system temporarily unavailable. No critical security issues detected based on available data.",
            processing_time=processing_time
        )


llama_ai_service = LlamaAIService()

async def analyze_token_with_ai(request: AIAnalysisRequest) -> Optional[AIAnalysisResponse]:
    """AI analysis that runs main + timing analysis separately then combines"""
    logger.info(f"üöÄ Running AI analysis for {request.token_address}")
    
    try:
        start_time = time.time()
        
        # Prepare data once for both analyses
        analysis_data = llama_ai_service._prepare_analysis_data(request)
        
        # Run both analyses concurrently
        main_task = run_main_analysis(analysis_data)
        timing_task = run_timing_analysis(analysis_data)
        
        main_result, timing_result = await asyncio.gather(main_task, timing_task, return_exceptions=True)
        
        # Handle main analysis result
        if isinstance(main_result, Exception) or not main_result:
            logger.error(f"Main analysis failed: {main_result}")
            return None
        
        # Handle timing analysis result
        if isinstance(timing_result, Exception) or not timing_result:
            logger.warning(f"Timing analysis failed: {timing_result}, using defaults")
            timing_result = {
                "last_pump": "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ",
                "next_window": "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ",
                "pump_probability": 50,
                "timing_confidence": 30,
                "market_phase": "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ",
                "reasoning": "–í—Ä–µ–º–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏–∑ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"
            }
        else:
            logger.info(f"‚úÖ Timing analysis succeeded: {timing_result}")
        
        # Combine results - Add timing to market_metrics (FIXED APPROACH)
        logger.info(f"üîó Combining main + timing results...")
        
        combined_market_metrics = main_result.market_metrics.copy()
        combined_market_metrics["timing_analysis"] = timing_result
        
        processing_time = time.time() - start_time
        
        combined_response = AIAnalysisResponse(
            ai_score=main_result.ai_score,
            risk_assessment=main_result.risk_assessment,
            recommendation=main_result.recommendation,
            confidence=main_result.confidence,
            key_insights=main_result.key_insights,
            risk_factors=main_result.risk_factors,
            stop_flags=main_result.stop_flags,
            market_metrics=combined_market_metrics,  # Contains timing_analysis
            llama_reasoning=main_result.llama_reasoning,
            processing_time=processing_time
        )
        
        logger.info(f"‚úÖ Combined AI analysis completed: Score {combined_response.ai_score}, Timing: {timing_result.get('next_window', 'Unknown')}")
        
        return combined_response
        
    except Exception as e:
        logger.error(f"Combined AI analysis failed: {e}")
        return None
    
async def run_main_analysis(analysis_data: Dict[str, Any]) -> Optional[AIAnalysisResponse]:
    """Run main risk analysis without timing"""
    try:
        # Build main analysis prompt (your existing prompt without timing section)
        prompt = llama_ai_service._build_main_analysis_prompt(analysis_data)
        
        # Call Groq
        response_text = await groq_llama_service.send_request(prompt)
        if not response_text:
            return None
        
        # Parse response
        response_data = json.loads(response_text)
        
        return AIAnalysisResponse(
            ai_score=float(response_data.get("ai_score", 60.0)),
            risk_assessment=response_data.get("risk_assessment", "medium"),
            recommendation=response_data.get("recommendation", "HOLD"),
            confidence=float(response_data.get("confidence", 70.0)),
            key_insights=response_data.get("key_insights", []),
            risk_factors=response_data.get("risk_factors", []),
            stop_flags=response_data.get("stop_flags", []),
            market_metrics=response_data.get("market_metrics", {}),
            llama_reasoning=response_data.get("llama_reasoning", "Analysis completed"),
            processing_time=0.0
        )
        
    except Exception as e:
        logger.error(f"Main analysis failed: {e}")
        return None

async def run_timing_analysis(analysis_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """Run timing analysis separately"""
    try:
        # Build timing-specific prompt using the new method
        timing_prompt = llama_ai_service._build_timing_analysis_prompt(analysis_data)
        
        # Call Groq
        response_text = await groq_llama_service.send_request(timing_prompt)
        if not response_text:
            return None
        
        # Parse timing response
        timing_data = json.loads(response_text)
        logger.info(f"‚úÖ Timing analysis: {timing_data.get('next_window', 'Unknown')}")
        return timing_data
        
    except Exception as e:
        logger.error(f"Timing analysis failed: {e}")
        return None

async def generate_analysis_docx_from_cache(cache_key: str) -> Optional[bytes]:
    """Generate DOCX report from cached analysis data"""
    try:
        logger.info(f"üìÑ Generating DOCX from cache key: {cache_key}")
        
        from app.utils.cache import cache_manager
        
        # Try to get cached data
        try:
            cached_data = await cache_manager.get(key=cache_key)
            if cached_data:
                logger.info(f"‚úÖ Found data in cache manager")
            else:
                logger.warning(f"‚ùå No data found in cache manager")
                return None
        except Exception as e:
            logger.error(f"Cache manager failed: {str(e)}")
            return None
        
        # Generate DOCX using the service
        return await docx_service.generate_analysis_docx_from_data(cached_data)
        
    except Exception as e:
        logger.error(f"‚ùå DOCX generation failed: {str(e)}")
        return None